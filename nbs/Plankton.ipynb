{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Bowl 2015\n",
    "\n",
    "## Organize Data\n",
    "\n",
    "- Create folder, then download files with kaggle-cli\n",
    "- data_dir should have subfolders train/ and test/ containing folders of images sorted by category, and 130400 test images, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys, shutil, random\n",
    "data_dir = '/home/ubuntu/fastai-data/plankton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organize directories\n",
    "%cd $data_dir\n",
    "%mkdir results\n",
    "%mkdir valid\n",
    "%mkdir temp\n",
    "%mkdir temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path variables\n",
    "train_path = os.path.join(data_dir,'train')\n",
    "valid_path = os.path.join(data_dir,'valid')\n",
    "test_path = os.path.join(data_dir,'test')\n",
    "result_path = os.path.join(data_dir,'results')\n",
    "temp_path = os.path.join(data_dir,'temp')\n",
    "temp_path2 = os.path.join(data_dir,'temp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code moves 1/5 of the total training images, selected at random, to the validation set. The distribution of both sets should be similar though slightly different, as this is a simple rather than stratified sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [os.path.join(valid_path,c,f) for c in os.listdir(train_path) for f in os.listdir(os.path.join(train_path,c))]\n",
    "files = random.sample(files, int(len(files)*0.2))\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.dirname(f)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(f))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    shutil.move(f.replace('/valid/','/train/'), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Move/Copy data into appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'test'\n",
    "%cd $data_dir\n",
    "%mv $name 'unknown'\n",
    "%mkdir $name\n",
    "%mv 'unknown' $name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thisdir = '/home/ubuntu/fastai-notes/nbs/'\n",
    "%cd $thisdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import MaxoutDense\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Instantiate plotting tool\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [forums](http://forums.fast.ai/t/batch-size-effect-on-validation-accuracy/413) batch sizes of at least 64 is better for approximating true gradient (and I don't want to go higher as it would necesitate simplifying the model or reducing the image resolution to fit in memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "batch_size = 128\n",
    "val_batch_size = batch_size*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation reduces overfitting and is an essential component of most modern neural nets. The rotation should have worked fine but I decided more extreme augmentation was not that helpful as my network was not overfitting much, vertical and horizontal flipping should end up making similar transformations, and excessive augmentation will reduce convergence speed.\n",
    "\n",
    "The relatively small image size is so my network can reliably run without memory allocation problems, as that was a problem on my p2.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24269 images belonging to 121 classes.\n",
      "Found 6067 images belonging to 121 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def get_batch2(dirname, temp_dir=None, augment=True, shuffle=True):\n",
    "    gen = image.ImageDataGenerator()\n",
    "    if augment:\n",
    "        gen = image.ImageDataGenerator(#rotation_range=360,\n",
    "                                       width_shift_range=0.05,\n",
    "                                       height_shift_range=0.05,\n",
    "                                       shear_range=0.10,\n",
    "                                       zoom_range=0.3,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True)\n",
    "\n",
    "    return gen.flow_from_directory(dirname,\n",
    "                                  target_size=(48,48),\n",
    "                                  class_mode='categorical',\n",
    "                                  color_mode='grayscale',\n",
    "                                  shuffle=shuffle,\n",
    "                                  save_to_dir=temp_dir,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "batches = get_batch2(train_path, temp_dir=None)\n",
    "val_batches = get_batch2(valid_path, temp_dir=None, shuffle=True, augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify augmentation\n",
    "\n",
    "When my model was preforming poorly (~5% validation accuracy and stagnant from there) I suspected there might be a problem with the validation set construction and so was careful to check that the augmentation did not create drastic differences between the training and validation sets.\n",
    "\n",
    "#### Check min/max/mean statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    batch, labels = next(batches) # save 3 batches of images\n",
    "    print('TRAINING | max: {} min: {} mean: {}'.format(np.max(batch), np.min(batch), np.mean(batch)))\n",
    "    val_batch, labels = next(val_batches)\n",
    "    print('VALIDATE | max: {} min: {} mean: {}\\n'.format(np.max(val_batch), np.min(val_batch), np.mean(val_batch)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-409d8f75e5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plots' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import random\n",
    "\n",
    "plots([image.load_img(os.path.join(temp_path, img)) for img in random.sample(os.listdir(temp_path), 8)])\n",
    "plots([image.load_img(os.path.join(temp_path2, img)) for img in random.sample(os.listdir(temp_path2), 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show distribution of images\n",
    "train_dist = [len(os.listdir(os.path.join(train_path,c))) for c in os.listdir(train_path)]\n",
    "valid_dist = [len(os.listdir(os.path.join(valid_path,c))) for c in os.listdir(valid_path)]\n",
    "train_sum = sum(train_dist)\n",
    "valid_sum = sum(valid_dist)\n",
    "for i in range(121):\n",
    "    print('{:5.3f}% training vs {:5.3f}% validation'\n",
    "          .format(100*train_dist[i]/float(train_sum), 100*valid_dist[i]/float(valid_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial model was based off a description of the [winning architecture](http://benanne.github.io/2015/03/17/plankton.html), without the rotation pooling and rolling parts. This did not lead to very good preformance - while the training accuracy would increase (though it wasn't that high, maybe 20%) the validation accuracy would be stagnant and very low (~5%).\n",
    "\n",
    "After [asking around](http://forums.fast.ai/t/low-accuracy-but-overfitting/5537/11) I was advised to try building a very simple architecture and see if it worked at all. To my suprise one 512-param dense layer between an input batchnorm and output dense(121) was all that was needed for pretty good accuracy up to 40%, though it was clearly underfitting (training accuracy up to 10% lower than validation).\n",
    "\n",
    "I then found a [simpler model](https://github.com/hjweide/NDSB/blob/master/net24.py) that achieved top ten, implemented in lasagne, and basically copied it in keras. This seems to work pretty well, with ~74% validation accuracy after 200 epochs at the default 0.001 learning rate (though the lr isn't that meaningful as I'm using Adam). Running for 300 epochs doesn't seem to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_1 (BatchNorma (None, 1, 48, 48)     2           batchnormalization_input_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 256, 45, 45)   4352        batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "prelu_1 (PReLU)                  (None, 256, 45, 45)   518400      convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 256, 45, 45)   512         prelu_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 256, 22, 22)   0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256, 22, 22)   0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 384, 20, 20)   885120      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "prelu_2 (PReLU)                  (None, 384, 20, 20)   153600      convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 384, 20, 20)   768         prelu_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 384, 10, 10)   0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 384, 10, 10)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 1024, 8, 8)    3539968     dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "prelu_3 (PReLU)                  (None, 1024, 8, 8)    65536       convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 1024, 8, 8)    2048        prelu_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 1024, 4, 4)    0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 1024, 4, 4)    0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16384)         0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxoutdense_1 (MaxoutDense)      (None, 1024)          33556480    flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "prelu_4 (PReLU)                  (None, 1024)          1024        maxoutdense_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 1024)          2048        prelu_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 1024)          0           batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxoutdense_2 (MaxoutDense)      (None, 1024)          2099200     dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "prelu_5 (PReLU)                  (None, 1024)          1024        maxoutdense_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 1024)          2048        prelu_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 1024)          0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 121)           124025      dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 40956155\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# super-simple model for testing\n",
    "# actually model copied from https://github.com/hjweide/NDSB/blob/master/net24.py\n",
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(1,48,48)),\n",
    "    \n",
    "    Convolution2D(256,4,4,init='he_uniform'), # convolution layers\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(strides=(2,2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Convolution2D(384,3,3,init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(strides=(2,2)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Convolution2D(1024,3,3,init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D(strides=(2,2)),\n",
    "    #Lambda(Maxout),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    MaxoutDense(1024, nb_feature=2, init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    MaxoutDense(1024, nb_feature=2, init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5,),\n",
    "    \n",
    "    Dense(121, activation='softmax', init='he_uniform')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7939 - acc: 0.7526 - val_loss: 0.9118 - val_acc: 0.7440\n",
      "Epoch 2/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7937 - acc: 0.7511 - val_loss: 0.9070 - val_acc: 0.7496\n",
      "Epoch 3/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7981 - acc: 0.7538 - val_loss: 0.9258 - val_acc: 0.7432\n",
      "Epoch 4/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.8011 - acc: 0.7477 - val_loss: 0.9096 - val_acc: 0.7397\n",
      "Epoch 5/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7933 - acc: 0.7522 - val_loss: 0.9262 - val_acc: 0.7416\n",
      "Epoch 6/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7831 - acc: 0.7516 - val_loss: 0.9077 - val_acc: 0.7432\n",
      "Epoch 7/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.8056 - acc: 0.7461 - val_loss: 0.8916 - val_acc: 0.7488\n",
      "Epoch 8/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7999 - acc: 0.7512 - val_loss: 0.9453 - val_acc: 0.7397\n",
      "Epoch 9/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7919 - acc: 0.7521 - val_loss: 0.8787 - val_acc: 0.7505\n",
      "Epoch 10/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7955 - acc: 0.7544 - val_loss: 0.9139 - val_acc: 0.7429\n",
      "Epoch 11/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.8007 - acc: 0.7499 - val_loss: 0.9274 - val_acc: 0.7422\n",
      "Epoch 12/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7792 - acc: 0.7562 - val_loss: 0.9052 - val_acc: 0.7483\n",
      "Epoch 13/100\n",
      "24269/24269 [==============================] - 225s - loss: 0.8041 - acc: 0.7503 - val_loss: 0.8664 - val_acc: 0.7551\n",
      "Epoch 14/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7869 - acc: 0.7537 - val_loss: 0.9299 - val_acc: 0.7409\n",
      "Epoch 15/100\n",
      "24269/24269 [==============================] - 225s - loss: 0.7959 - acc: 0.7533 - val_loss: 0.9172 - val_acc: 0.7427\n",
      "Epoch 16/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7976 - acc: 0.7542 - val_loss: 0.8976 - val_acc: 0.7465\n",
      "Epoch 17/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7853 - acc: 0.7535 - val_loss: 0.9304 - val_acc: 0.7397\n",
      "Epoch 18/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7878 - acc: 0.7528 - val_loss: 0.9260 - val_acc: 0.7384\n",
      "Epoch 19/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7812 - acc: 0.7548 - val_loss: 0.9151 - val_acc: 0.7434\n",
      "Epoch 20/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7880 - acc: 0.7540 - val_loss: 0.9057 - val_acc: 0.7445\n",
      "Epoch 21/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7835 - acc: 0.7536 - val_loss: 0.9076 - val_acc: 0.7455\n",
      "Epoch 22/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7933 - acc: 0.7499 - val_loss: 0.9163 - val_acc: 0.7429\n",
      "Epoch 23/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7941 - acc: 0.7543 - val_loss: 0.9201 - val_acc: 0.7434\n",
      "Epoch 24/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7962 - acc: 0.7496 - val_loss: 0.8973 - val_acc: 0.7486\n",
      "Epoch 25/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7908 - acc: 0.7535 - val_loss: 0.9094 - val_acc: 0.7442\n",
      "Epoch 26/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7910 - acc: 0.7577 - val_loss: 0.9050 - val_acc: 0.7417\n",
      "Epoch 27/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7919 - acc: 0.7547 - val_loss: 0.8995 - val_acc: 0.7501\n",
      "Epoch 28/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7866 - acc: 0.7516 - val_loss: 0.8883 - val_acc: 0.7452\n",
      "Epoch 29/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7850 - acc: 0.7532 - val_loss: 0.9050 - val_acc: 0.7444\n",
      "Epoch 30/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7931 - acc: 0.7533 - val_loss: 0.9066 - val_acc: 0.7427\n",
      "Epoch 31/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7929 - acc: 0.7540 - val_loss: 0.9191 - val_acc: 0.7432\n",
      "Epoch 32/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7955 - acc: 0.7523 - val_loss: 0.9018 - val_acc: 0.7448\n",
      "Epoch 33/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.8059 - acc: 0.7512 - val_loss: 0.8993 - val_acc: 0.7467\n",
      "Epoch 34/100\n",
      "24269/24269 [==============================] - 231s - loss: 0.7874 - acc: 0.7580 - val_loss: 0.9167 - val_acc: 0.7396\n",
      "Epoch 35/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7986 - acc: 0.7552 - val_loss: 0.9102 - val_acc: 0.7444\n",
      "Epoch 36/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7796 - acc: 0.7551 - val_loss: 0.8986 - val_acc: 0.7422\n",
      "Epoch 37/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7971 - acc: 0.7503 - val_loss: 0.9100 - val_acc: 0.7439\n",
      "Epoch 38/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7948 - acc: 0.7506 - val_loss: 0.8964 - val_acc: 0.7495\n",
      "Epoch 39/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7835 - acc: 0.7561 - val_loss: 0.9129 - val_acc: 0.7388\n",
      "Epoch 40/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7878 - acc: 0.7550 - val_loss: 0.9077 - val_acc: 0.7404\n",
      "Epoch 41/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7859 - acc: 0.7535 - val_loss: 0.9170 - val_acc: 0.7450\n",
      "Epoch 42/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7923 - acc: 0.7533 - val_loss: 0.8829 - val_acc: 0.7448\n",
      "Epoch 43/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7811 - acc: 0.7525 - val_loss: 0.8993 - val_acc: 0.7453\n",
      "Epoch 44/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7918 - acc: 0.7552 - val_loss: 0.9100 - val_acc: 0.7447\n",
      "Epoch 45/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7933 - acc: 0.7506 - val_loss: 0.9152 - val_acc: 0.7442\n",
      "Epoch 46/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7817 - acc: 0.7549 - val_loss: 0.8904 - val_acc: 0.7472\n",
      "Epoch 47/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7826 - acc: 0.7547 - val_loss: 0.8938 - val_acc: 0.7475\n",
      "Epoch 48/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7845 - acc: 0.7554 - val_loss: 0.9174 - val_acc: 0.7379\n",
      "Epoch 49/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7893 - acc: 0.7544 - val_loss: 0.9028 - val_acc: 0.7440\n",
      "Epoch 50/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7923 - acc: 0.7515 - val_loss: 0.9003 - val_acc: 0.7458\n",
      "Epoch 51/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7890 - acc: 0.7524 - val_loss: 0.8999 - val_acc: 0.7465\n",
      "Epoch 52/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7868 - acc: 0.7556 - val_loss: 0.9268 - val_acc: 0.7358\n",
      "Epoch 53/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7908 - acc: 0.7551 - val_loss: 0.9116 - val_acc: 0.7435\n",
      "Epoch 54/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.8000 - acc: 0.7526 - val_loss: 0.9136 - val_acc: 0.7442\n",
      "Epoch 55/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7882 - acc: 0.7531 - val_loss: 0.8771 - val_acc: 0.7452\n",
      "Epoch 56/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7922 - acc: 0.7546 - val_loss: 0.9461 - val_acc: 0.7336\n",
      "Epoch 57/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7842 - acc: 0.7566 - val_loss: 0.8715 - val_acc: 0.7519\n",
      "Epoch 58/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7777 - acc: 0.7522 - val_loss: 0.9142 - val_acc: 0.7417\n",
      "Epoch 59/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7886 - acc: 0.7536 - val_loss: 0.9089 - val_acc: 0.7437\n",
      "Epoch 60/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7868 - acc: 0.7545 - val_loss: 0.9032 - val_acc: 0.7444\n",
      "Epoch 61/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7918 - acc: 0.7535 - val_loss: 0.9176 - val_acc: 0.7457\n",
      "Epoch 62/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7851 - acc: 0.7545 - val_loss: 0.9052 - val_acc: 0.7437\n",
      "Epoch 63/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7872 - acc: 0.7577 - val_loss: 0.9147 - val_acc: 0.7419\n",
      "Epoch 64/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7889 - acc: 0.7546 - val_loss: 0.9011 - val_acc: 0.7444\n",
      "Epoch 65/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7895 - acc: 0.7546 - val_loss: 0.8928 - val_acc: 0.7457\n",
      "Epoch 66/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7943 - acc: 0.7508 - val_loss: 0.9009 - val_acc: 0.7435\n",
      "Epoch 67/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7879 - acc: 0.7541 - val_loss: 0.9013 - val_acc: 0.7448\n",
      "Epoch 68/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7919 - acc: 0.7517 - val_loss: 0.9001 - val_acc: 0.7439\n",
      "Epoch 69/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7893 - acc: 0.7578 - val_loss: 0.9155 - val_acc: 0.7440\n",
      "Epoch 70/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7829 - acc: 0.7548 - val_loss: 0.8951 - val_acc: 0.7496\n",
      "Epoch 71/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7951 - acc: 0.7506 - val_loss: 0.8958 - val_acc: 0.7480\n",
      "Epoch 72/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7920 - acc: 0.7538 - val_loss: 0.9230 - val_acc: 0.7402\n",
      "Epoch 73/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7898 - acc: 0.7558 - val_loss: 0.9013 - val_acc: 0.7442\n",
      "Epoch 74/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7910 - acc: 0.7521 - val_loss: 0.9049 - val_acc: 0.7414\n",
      "Epoch 75/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7875 - acc: 0.7535 - val_loss: 0.8947 - val_acc: 0.7465\n",
      "Epoch 76/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7980 - acc: 0.7544 - val_loss: 0.8840 - val_acc: 0.7516\n",
      "Epoch 77/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7867 - acc: 0.7548 - val_loss: 0.9136 - val_acc: 0.7404\n",
      "Epoch 78/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7874 - acc: 0.7574 - val_loss: 0.8943 - val_acc: 0.7434\n",
      "Epoch 79/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.8043 - acc: 0.7530 - val_loss: 0.9024 - val_acc: 0.7452\n",
      "Epoch 80/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7903 - acc: 0.7542 - val_loss: 0.9110 - val_acc: 0.7465\n",
      "Epoch 81/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7787 - acc: 0.7596 - val_loss: 0.9213 - val_acc: 0.7427\n",
      "Epoch 82/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7800 - acc: 0.7561 - val_loss: 0.8809 - val_acc: 0.7465\n",
      "Epoch 83/100\n",
      "24269/24269 [==============================] - 225s - loss: 0.7832 - acc: 0.7528 - val_loss: 0.9052 - val_acc: 0.7437\n",
      "Epoch 84/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7824 - acc: 0.7515 - val_loss: 0.8945 - val_acc: 0.7493\n",
      "Epoch 85/100\n",
      "24269/24269 [==============================] - 226s - loss: 0.7890 - acc: 0.7555 - val_loss: 0.9178 - val_acc: 0.7383\n",
      "Epoch 86/100\n",
      "24269/24269 [==============================] - 227s - loss: 0.7880 - acc: 0.7542 - val_loss: 0.9005 - val_acc: 0.7463\n",
      "Epoch 87/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7784 - acc: 0.7542 - val_loss: 0.8974 - val_acc: 0.7417\n",
      "Epoch 88/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7953 - acc: 0.7514 - val_loss: 0.8969 - val_acc: 0.7440\n",
      "Epoch 89/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7889 - acc: 0.7519 - val_loss: 0.9226 - val_acc: 0.7435\n",
      "Epoch 90/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7821 - acc: 0.7587 - val_loss: 0.8800 - val_acc: 0.7480\n",
      "Epoch 91/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7973 - acc: 0.7501 - val_loss: 0.8902 - val_acc: 0.7439\n",
      "Epoch 92/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7830 - acc: 0.7537 - val_loss: 0.9001 - val_acc: 0.7453\n",
      "Epoch 93/100\n",
      "24269/24269 [==============================] - 230s - loss: 0.7848 - acc: 0.7536 - val_loss: 0.8885 - val_acc: 0.7490\n",
      "Epoch 94/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7858 - acc: 0.7545 - val_loss: 0.8957 - val_acc: 0.7486\n",
      "Epoch 95/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7851 - acc: 0.7550 - val_loss: 0.9254 - val_acc: 0.7406\n",
      "Epoch 96/100\n",
      "24269/24269 [==============================] - 228s - loss: 0.7861 - acc: 0.7535 - val_loss: 0.8901 - val_acc: 0.7495\n",
      "Epoch 97/100\n",
      "24269/24269 [==============================] - 231s - loss: 0.7885 - acc: 0.7544 - val_loss: 0.9008 - val_acc: 0.7447\n",
      "Epoch 98/100\n",
      "24269/24269 [==============================] - 231s - loss: 0.7842 - acc: 0.7569 - val_loss: 0.8629 - val_acc: 0.7518\n",
      "Epoch 99/100\n",
      "24269/24269 [==============================] - 231s - loss: 0.7791 - acc: 0.7557 - val_loss: 0.9063 - val_acc: 0.7477\n",
      "Epoch 100/100\n",
      "24269/24269 [==============================] - 229s - loss: 0.7819 - acc: 0.7581 - val_loss: 0.9261 - val_acc: 0.7439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb960fc4710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_epochs = 100\n",
    "model.optimizer.lr = .000001\n",
    "\n",
    "model.load_weights(os.path.join(result_path,'final_weights_210epochs.hf5'))\n",
    "model.fit_generator(batches,\n",
    "                samples_per_epoch=batches.nb_sample,\n",
    "                validation_data=val_batches,\n",
    "                nb_val_samples=val_batches.nb_sample,\n",
    "                nb_epoch=no_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "model.save_weights(os.path.join(result_path,'final_weights_310epochs.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130400 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = image.ImageDataGenerator().flow_from_directory(test_path,\n",
    "                                                             target_size=(48,48),\n",
    "                                                             class_mode='categorical',\n",
    "                                                             color_mode='grayscale',\n",
    "                                                             shuffle=False,\n",
    "                                                             batch_size=batch_size)\n",
    "\n",
    "model.load_weights(os.path.join(result_path,'final_weights_310epochs.hf5'))\n",
    "preds = model.predict_generator(test_batches, test_batches.nb_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130400, 121)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1.jpg', '1.11927320177e-05', '1.69221993189e-12', '1.67455866062e-07', '2.5532399377e-06',\n",
       "       '3.67453935723e-07', '0.000899883802049', '0.00030377987423', '0.000209333928069',\n",
       "       '0.00170310190879', '1.52098629915e-13', '0.000106084356958', '0.0018167673843',\n",
       "       '0.000120061762573', '1.62218293553e-06', '1.0717430996e-05', '6.69723476676e-06',\n",
       "       '1.68615414964e-07', '7.36322181183e-06', '4.63979773713e-07', '5.69406938666e-06',\n",
       "       '6.57979171592e-06', '1.14658313066e-07', '1.46009128343e-07', '4.34908198343e-09',\n",
       "       '6.18126941845e-05', '7.13295003152e-06', '8.95665380085e-06', '0.000146111735376',\n",
       "       '1.42830538152e-06', '3.3244871247e-06', '2.34814501709e-06', '2.8121508594e-11',\n",
       "       '9.34020044951e-06', '0.000768653932028', '0.0164761170745', '0.0242573413998',\n",
       "       '0.0630942136049', '0.331187844276', '2.76640328423e-08', '9.00033483049e-05',\n",
       "       '4.79042849832e-09', '2.73407447793e-08', '1.49117404362e-05', '5.08299035573e-06',\n",
       "       '2.02384558179e-06', '1.9575516555e-11', '2.75089996925e-17', '2.63787092081e-06',\n",
       "       '1.5850064301e-05', '0.507348239422', '6.14435531534e-14', '9.80443548571e-09',\n",
       "       '1.10023665911e-06', '1.30036653445e-06', '5.97937560087e-06', '2.9878051464e-06',\n",
       "       '6.66409150085e-10', '1.26350084884e-06', '5.32790842556e-08', '7.34702165772e-09',\n",
       "       '1.38437155783e-06', '1.4331598408e-10', '3.94707289075e-09', '4.53550086377e-10',\n",
       "       '4.17439077864e-05', '8.27002804726e-06', '4.69236383083e-09', '3.46972797161e-06',\n",
       "       '5.38196161415e-07', '1.2868167687e-05', '3.59800219485e-06', '1.32515310725e-06',\n",
       "       '6.1434957388e-06', '2.35621058664e-07', '1.47119134652e-08', '6.97461954857e-09',\n",
       "       '2.65950521154e-12', '1.57645445142e-05', '2.70870148533e-09', '2.98217166517e-08',\n",
       "       '1.91498475033e-05', '9.49549357756e-05', '3.70785278392e-06', '2.78366110251e-06',\n",
       "       '9.1249930847e-06', '0.000276167062111', '3.87205727748e-06', '2.15854493035e-06',\n",
       "       '1.17207840655e-10', '9.62637045632e-07', '0.00125257158652', '4.51655942015e-05',\n",
       "       '1.4200075384e-05', '2.56318543279e-07', '3.39235293723e-06', '2.21849550144e-05',\n",
       "       '4.16794046032e-06', '1.12871498459e-06', '6.42138502371e-06', '2.91220908366e-06',\n",
       "       '1.41782152241e-06', '6.43834027869e-06', '1.99741134566e-06', '3.96752009024e-09',\n",
       "       '3.57147018804e-06', '1.14872726442e-07', '4.35694119005e-07', '1.19470664472e-08',\n",
       "       '0.000545146176592', '1.46042964388e-07', '4.47849924967e-06', '0.00389192136936',\n",
       "       '8.58344173515e-10', '4.30298750871e-05', '2.42023888859e-05', '3.29010589439e-07',\n",
       "       '6.06504329426e-07', '4.05621358368e-06', '0.00319515750743', '0.0415116921067',\n",
       "       '0.000175573353772'],\n",
       "      dtype='|S32')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [os.path.split(f)[-1] for f in test_batches.filenames]\n",
    "ids = np.expand_dims(ids, axis=1)\n",
    "print(preds.shape)\n",
    "subm = np.hstack([ids,preds])\n",
    "subm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr0AAABpCAYAAAA3FJvIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8d0dc2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAFUCAYAAABLHtWUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBGj6PwXVMZcL7r5fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb954b1b2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "plots([image.load_img(os.path.join(train_path, 'trichodesmium_puff', img)) for img in random.sample(os.listdir(os.path.join(train_path, 'trichodesmium_puff')), 8)])\n",
    "plots([image.load_img(os.path.join(test_path, 'unknown', '10000.jpg'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='plankton_subm2.csv' target='_blank'>plankton_subm2.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fastai-notes/nbs/plankton_subm2.csv"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "subm_filename = 'plankton_subm2.csv'\n",
    "classes = [i[0] for i in sorted(batches.class_indices.items(), key=operator.itemgetter(1))]\n",
    "header='image,'+','.join(classes)\n",
    "\n",
    "np.savetxt(subm_filename, subm, fmt='%s', delimiter=',', header=header, comments='')\n",
    "FileLink(subm_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_epochs = 5\n",
    "model.optimizer.lr = 0.00025\n",
    "model.load_weights(os.path.join(result_path,'weights3_epoch6.hf5'))\n",
    "for i in range(6, no_epochs+6):\n",
    "    model.fit_generator(batches,\n",
    "                    samples_per_epoch=batches.nb_sample,\n",
    "                    validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample,\n",
    "                    nb_epoch=1)\n",
    "    weights_filename = 'weights3_epoch{}.hf5'.format(i+1)\n",
    "    model.save_weights(os.path.join(result_path,weights_filename))\n",
    "    model.optimizer.lr *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_epochs = 30\n",
    "model.load_weights(os.path.join(result_path,'weights3_epoch5.hf5'))\n",
    "model.optimizer.lr = 0.00000001\n",
    "for i in range(10, no_epochs+10):\n",
    "    model.fit_generator(batches,\n",
    "                    samples_per_epoch=batches.nb_sample,\n",
    "                    validation_data=val_batches,\n",
    "                    nb_val_samples=val_batches.nb_sample,\n",
    "                    nb_epoch=1)\n",
    "    weights_filename = 'weights3_epoch{}.hf5'.format(i+1)\n",
    "    model.save_weights(os.path.join(result_path,weights_filename))\n",
    "    model.optimizer.lr *= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def Maxout(x, num_unit=None):\n",
    "    # Maxout as in the paper `Maxout Networks <http://arxiv.org/abs/1302.4389>`_.\n",
    "    input_shape = x._keras_shape\n",
    "    ndim = len(input_shape)\n",
    "    assert ndim == 4 or ndim == 2\n",
    "\n",
    "    data_format = K.image_dim_ordering()\n",
    "\n",
    "    if data_format == 'th':\n",
    "        ch = input_shape[1]\n",
    "    else:\n",
    "        ch = input_shape[-1]\n",
    "\n",
    "    if num_unit == None:\n",
    "        num_unit = ch / 2\n",
    "    assert ch is not None and ch % num_unit == 0\n",
    "\n",
    "    if ndim == 4:\n",
    "        if data_format == 'th':\n",
    "            x = K.permute_dimensions(x, (0, 2, 3, 1))\n",
    "        x = K.reshape(x, (-1, input_shape[2], input_shape[3], ch / num_unit, num_unit))\n",
    "        x = K.max(x, axis=3)\n",
    "        if data_format == 'th':\n",
    "            x = K.permute_dimensions(x, (0, 3, 1, 2))\n",
    "    else:\n",
    "        print('1.')\n",
    "        x = K.reshape(x, (-1, ch / num_unit, num_unit))\n",
    "        print('2.')\n",
    "        x = K.max(x, axis=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Old model (doesn't work well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(1,224,224)), # automatically normalize inputs\n",
    "    Convolution2D(32,3,3, border_mode='same', init='he_uniform'), # convolution layers\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(32,1,1, border_mode='same', init='he_uniform'), # depthwise seperate?\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(16,3,3, border_mode='same', init='he_uniform'),\n",
    "    MaxPooling2D((4,4)), # max pooling\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Dropout(0.2),\n",
    "    Convolution2D(64,3,3, border_mode='same', init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(128,3,3, border_mode='same', init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(128,1,1, border_mode='same', init='he_uniform'), # depthwise seperate?\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    Convolution2D(64,3,3, border_mode='same', init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(axis=1),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, init='he_uniform'),\n",
    "    PReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(121, activation='softmax', init='he_uniform')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
