{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.bulbagarden.net/upload/2/21/001Bulbasaur.png\" alt=\"Bulbasaur Pic\" style=\"width: 256px;\"/>\n",
    "\n",
    "# Pokemon Image Generation GAN\n",
    "\n",
    "## Download from pkparaiso\n",
    "1. Install unzip and imagemagick\n",
    "2. Create fastai-data/pokemon_img/ and move to it\n",
    "3. Copy paste the section below into a bash script and run it\n",
    "4. Run the section below that in jupyter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/sh\n",
    "\n",
    "# download zipped gifs from pkparaiso\n",
    "wget https://www.pkparaiso.com/xy/xy-1291-animated-gifs-7bdab16fdede0fa327e2c04a6545ea4f.zip -O gifs.zip\n",
    "# unzip download\n",
    "unzip -q gifs.zip -d gifs\n",
    "\n",
    "# split gif into pngs\n",
    "mkdir pngs\n",
    "for img in gifs/*.gif; do\n",
    "    convert $img pngs/$(basename $img .gif)-%02d.png\n",
    "done\n",
    "\n",
    "# whiten background and trim\n",
    "for img in pngs/*.png; do\n",
    "    convert $img -background white -alpha remove -trim +repage \\\n",
    "    -thumbnail 248x248 -gravity center -extent 256x256 $img\n",
    "done\n",
    "\n",
    "# create other folders\n",
    "mkdir results\n",
    "mkdir temp\n",
    "mkdir train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/nidoran_f.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/lugiad.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/farfetchd.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/flabebe-orange.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/nidoran_m.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/flabebe-blue.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/volcanion.gif\n",
      "Error on /home/ubuntu/fastai-data/pokemon_img/gifs/flabebe-white.gif\n"
     ]
    }
   ],
   "source": [
    "import glob, shutil, json, collections\n",
    "import requests, imagehash\n",
    "from PIL import Image\n",
    "\n",
    "data_dir = '/home/ubuntu/fastai-data/pokemon_img'\n",
    "\n",
    "for gif in glob.glob(os.path.join(data_dir, 'gifs', '*.gif')):\n",
    "    name = gif.split('/')[-1][:-4].split('-')[0]\n",
    "    response = requests.get('https://api.pokemontcg.io/v1/cards?name={}&supertype=pokemon'.format(name))\n",
    "    hashes = []\n",
    "    try:\n",
    "        types = json.loads(response.content.decode('utf-8'))['cards'][-1]['types'][0]\n",
    "        os.makedirs(os.path.join(data_dir, 'train', types), exist_ok=True)\n",
    "        for png in glob.glob(os.path.join(data_dir, 'pngs', '{}*.png'.format(name))):\n",
    "            _hash = imagehash.dhash(Image.open(png))\n",
    "            if _hash not in hashes:\n",
    "                hash\n",
    "                shutil.move(png, os.path.join(data_dir, 'train', types))\n",
    "    except (KeyError, IndexError):\n",
    "        print('Error on '+gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GAN\n",
    "1. When running as python script use the following commands"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jupyter nbconvert --to script Pokemon_Img.ipynb --TemplateExporter.exclude_markdown=True --TemplateExporter.exclude_raw=True\n",
    "\n",
    "python3 Pokemon_Img.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from itertools import product\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.initializers import *\n",
    "from keras.layers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.optimizers import Adam\n",
    "import itertools, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defines some layer functions for use in generator and discriminator networks\n",
    "- pixel shuffle from https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/blob/master/models.py\n",
    "- Appears that changing to upscale + convolution instead of deconvolution (convolution + pixel shuffle) improves diversity of results as well as eliminates checkerboard artifacts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block for generative network\n",
    "def g_resblock(x, filters=32):\n",
    "    skip = x\n",
    "    \n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, skip])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# residual block for discriminator network\n",
    "def d_resblock(x, filters=16, strides=1):\n",
    "    skip = x\n",
    "    \n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Conv2D(filters, 3, padding='same', strides=strides)(x)\n",
    "    x = Add()([x, skip])\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# superresolution block\n",
    "def sp_block(x, filters=16):\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates a super-resolution resnet like generator model, and a resnet discriminator model, approx. according to the [technical report](https://makegirlsmoe.github.io/assets/pdf/technical_report.pdf)\n",
    "- generator model has less resblocks to reduce complexity and make it not converge as fast\n",
    "- looks more random if activations are removed from sp_cnn blocks?\n",
    "- Using tanh instead of relu and family seems to make things better as well?\n",
    "- Adding gaussian noise to input of discriminator might stabilize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of noise vector\n",
    "z_length = 100\n",
    "# number of pokemon types\n",
    "num_types = 11\n",
    "\n",
    "# generator network architecture\n",
    "def get_generator(x, y, dim=16, depth=16):\n",
    "    y = Dense(z_length)(y)\n",
    "    x = Multiply()([x, y]) # combine noise and type\n",
    "    \n",
    "    x = Dense(dim*dim*depth)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Reshape((dim, dim, depth))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    skip = x = Conv2D(1, 1, padding='same')(x)\n",
    "\n",
    "    for i in range(20):\n",
    "        x = g_resblock(x)\n",
    "        \n",
    "    x = Conv2D(32, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Add()([x, skip])\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = sp_block(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(3, 9, padding='same')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# discriminator network architecture\n",
    "def get_discriminator(x):\n",
    "    x = Conv2D(16, 4, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    for i in range(3):\n",
    "        x = d_resblock(x, filters=2**(4+i))\n",
    "        x = d_resblock(x, filters=2**(4+i))\n",
    "        if (i < 2):\n",
    "            x = Conv2D(2**(5+i), 4, strides=4, padding='same')(x)\n",
    "        else:\n",
    "            x = Dropout(0.4)(x)\n",
    "            x = Conv2D(2**(5+i), 3, strides=2, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(32, 4, strides=4, padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    y = Dense(11)(x) # classify type of pokemon\n",
    "    y = Activation('softmax')(y)\n",
    "    x = Dense(1)(x) # classify real or fake\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.9339378476142883 min: -0.8952038884162903 mean: 0.11079838871955872\n"
     ]
    }
   ],
   "source": [
    "g_input = Input(shape=[z_length], name='noise')\n",
    "g_input2 = Input(shape=[num_types], name='type')\n",
    "generator = Model(inputs=[g_input, g_input2], outputs=get_generator(g_input, g_input2))\n",
    "\n",
    "g_test = generator.predict({'noise': np.random.normal(0, 1, size=(8, z_length)),\n",
    "                            'type': np.eye(num_types)[np.random.choice(num_types, 8)]})\n",
    "print('max: {} min: {} mean: {}'.format(np.max(g_test), np.min(g_test), np.mean(g_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.5586484670639038 min: 0.536372184753418 mean: 0.5456167459487915\n"
     ]
    }
   ],
   "source": [
    "d_input = Input(shape=[256, 256, 3], name='images')\n",
    "discriminator = Model(inputs=d_input, outputs=get_discriminator(d_input))\n",
    "\n",
    "d_test, d_test2 = discriminator.predict({'images': g_test})\n",
    "print('max: {} min: {} mean: {}'.format(np.max(d_test), np.min(d_test), np.mean(d_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converts generator and discriminator networks into the form required for DRAGAN training\n",
    "    - adds nontrainable discriminator network layers to generator, to allow us to maximize discriminator loss\n",
    "    - adds other loss functions and inputs to discriminator network, so it trains on real examples as well as generated examples, and uses the DRAGAN gradient penalty\n",
    "    - generator should have lower learning rate than discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_33 (Model)                (None, 256, 256, 3)  820132      input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 256, 256, 3)  0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_34 (Model)                [(None, 1), (None, 1 375644      input_19[0][0]                   \n",
      "                                                                 model_33[1][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,195,776\n",
      "Trainable params: 375,644\n",
      "Non-trainable params: 820,132\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "# discriminator_model trains discriminator with real and generated images\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = True\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = False\n",
    "discriminator.trainable = True\n",
    "generator.trainable = False\n",
    "\n",
    "# blog article suggests perturbing in all directions (-1 to 1 rather than 0 to 1)\n",
    "def perturb(input, c=0.1):\n",
    "    b, row, col, k = batch_size, 256, 256, 3 #input.shape\n",
    "    alpha = K.repeat_elements(K.repeat_elements(K.repeat_elements(\n",
    "        K.random_uniform((b, 1, 1, 1), 0, 1), row, 1), col, 2), k, 3)\n",
    "    x_hat = alpha*input + (1-alpha)*(input + c * K.std(input) * K.random_uniform((b, row, col, k), 0, 1))\n",
    "    return x_hat\n",
    "\n",
    "# inputs for stacked generator/discriminator\n",
    "imgs = Input(shape=[256, 256, 3]) # real mini-batch\n",
    "p_imgs = Lambda(perturb, output_shape=K.int_shape(imgs)[1:])(imgs) # perturbed mini-batch\n",
    "noise = Input(shape=[z_length])\n",
    "types = Input(shape=[num_types])\n",
    "\n",
    "# from keras WGAN-GP, though called with randomly perturbed inputs rather than averaged inputs\n",
    "def gradient_penalty(y_true, y_pred, x_hat=p_imgs):\n",
    "    gradients = K.gradients(K.sum(y_pred), x_hat)\n",
    "    gradient_l2_norm = K.sqrt(K.sum(K.square(gradients), axis=[1,2,3]))\n",
    "    gradient_penalty = K.mean(K.square(gradient_l2_norm - 1))\n",
    "    return gradient_penalty\n",
    "\n",
    "# discriminator model with inputs both real and generated images\n",
    "discriminator_model = Model(inputs=[imgs, noise, types],\n",
    "                    outputs=discriminator(imgs)+discriminator(generator([noise, types]))+discriminator(p_imgs))\n",
    "discriminator_model.compile(optimizer=Adam(3e-4),\n",
    "                    loss=['binary_crossentropy', 'categorical_crossentropy',\n",
    "                          'binary_crossentropy', 'categorical_crossentropy',\n",
    "                          gradient_penalty, gradient_penalty],\n",
    "                    loss_weights=[1.8, 0.2, 0.9, 0.1, 1.0, 0])\n",
    "\n",
    "#test2 = discriminator_model.predict([generator.predict(np.random.normal(size=(4, 128))),\n",
    "#                                     np.random.normal(size=(4, 128))])\n",
    "#print('max: {} min: {} mean: {}'.format(np.max(test2[1]), np.min(test2[1]), np.mean(test2[1])))\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_33 (Model)                (None, 256, 256, 3)  820132      input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_34 (Model)                [(None, 1), (None, 1 375644      model_33[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,195,776\n",
      "Trainable params: 809,220\n",
      "Non-trainable params: 386,556\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# generator_model trains generator to create image, optimizes to maximize discriminator loss\n",
    "for layer in discriminator.layers:\n",
    "    layer.trainable = False\n",
    "for layer in generator.layers:\n",
    "    layer.trainable = True\n",
    "discriminator.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "generator_model = Model(inputs=[noise, types], outputs=discriminator(generator([noise, types])))\n",
    "generator_model.compile(optimizer=Adam(1e-3),\n",
    "                        loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "                        loss_weights=[0.9, 0.1])\n",
    "\n",
    "#test = generator_model.predict(np.random.normal(size=(4, 256)))\n",
    "#print('max: {} min: {} mean: {}'.format(np.max(test), np.min(test), np.mean(test)))\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your DRAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random, os\n",
    "\n",
    "# Instantiate plotting tool\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(12,6), rows=2, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/ubuntu/fastai-data/pokemon_img'\n",
    "train_path = os.path.join(data_dir, 'train')\n",
    "temp_path = os.path.join(data_dir, 'temp')\n",
    "result_path = os.path.join(data_dir, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76842 images belonging to 11 classes.\n",
      "mean: 0.040253426879644394 std dev: 0.99752277135849\n"
     ]
    }
   ],
   "source": [
    "def get_batches(dirname, temp_dir=None, shuffle=True, batch_size=batch_size):\n",
    "    gen = image.ImageDataGenerator(preprocessing_function=lambda x: (x - 226)/57,\n",
    "                                  horizontal_flip=True,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  zoom_range=[1.0, 1.65],\n",
    "                                  fill_mode='constant',\n",
    "                                  cval=255)\n",
    "    return gen.flow_from_directory(dirname,\n",
    "                                  target_size=(256,256),\n",
    "                                  class_mode='categorical',\n",
    "                                  color_mode='rgb',\n",
    "                                  shuffle=shuffle,\n",
    "                                  save_to_dir=temp_dir,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "#batches = get_batches(train_path, temp_dir=temp_path)\n",
    "batches = get_batches(train_path)\n",
    "\n",
    "batch, labels = next(batches)\n",
    "print('mean: {} std dev: {}'.format(np.mean(batch), np.std(batch)))\n",
    "#plots([image.load_img(os.path.join(temp_path, img)) for img in random.sample(os.listdir(temp_path), 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def tile_images(image_stack):\n",
    "    assert len(image_stack.shape) == 4\n",
    "    image_list = [image_stack[i, :, :, :] for i in range(image_stack.shape[0])]\n",
    "    tiled_images = np.concatenate(image_list, axis=1)\n",
    "    return tiled_images\n",
    "\n",
    "def generate_images(generator, output_dir, epoch):\n",
    "    test_image_stack = generator.predict([np.random.normal(0, 1, size=(8, z_length)), np.eye(num_types)[:8]]) \n",
    "    test_image_stack = (test_image_stack * 57) + 226\n",
    "    test_image_stack = np.squeeze(np.round(test_image_stack).astype(np.uint8))\n",
    "    tiled_output = tile_images(test_image_stack)\n",
    "    tiled_output = Image.fromarray(tiled_output, mode='RGB')\n",
    "    outfile = os.path.join(output_dir, 'epoch_{}.png'.format(epoch))\n",
    "    tiled_output.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concensus from [Animeface-GAN](https://github.com/forcecore/Keras-GAN-Animeface-Character) and others seems to be generator loss of 2-4 and discriminator loss around 0.3 is good\n",
    "- One sided label noising and both sided label smoothing for discriminator, no smoothing or noising for generator seems to work well for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "sample_period = int(38376 / batch_size / 4) # sample images and print losses every 1/4 epoch\n",
    "save_period = int(38376 / batch_size / 20) # save weights every 1/20 epoch\n",
    "true_positive_y = np.ones((batch_size, 1))\n",
    "true_negative_y = np.zeros((batch_size, 1))\n",
    "dummy_y = np.zeros((batch_size, 1)) + 0.5\n",
    "\n",
    "epoch = 0\n",
    "if epoch > 0:\n",
    "    generator_model.load_weights(os.path.join(result_path, 'gan_g_weights{}.h5'.format(epoch)))\n",
    "    discriminator_model.load_weights(os.path.join(result_path, 'gan_d_weights{}.h5'.format(epoch)))\n",
    "    \"\"\"\n",
    "    discriminator_loss = np.loadtxt(os.path.join(result_path, 'gan_d_loss_history.csv'))\n",
    "    discriminator_loss = list(np.broadcast_to(\n",
    "        np.expand_dims(discriminator_loss, axis=1),(discriminator_loss.shape[0], 4)))\n",
    "    generator_loss =  list(np.loadtxt(os.path.join(result_path, 'gan_g_loss_history.csv')))\n",
    "    batch_num = len(discriminator_loss) + 1\n",
    "    \"\"\"\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    batch_num = 0\n",
    "else:\n",
    "    discriminator_loss = []\n",
    "    generator_loss = []\n",
    "    batch_num = 0\n",
    "\n",
    "epoch += 1\n",
    "print('Epoch ' + str(epoch) + '\\n')\n",
    "for imgs, types in batches:\n",
    "    \n",
    "    # save weights and generate sample images every so often\n",
    "    if batch_num % save_period == 0 and batch_num > 1:\n",
    "        print('D. Loss | Total: ' + str(discriminator_loss[-1][0])\n",
    "              + ' | Real: ' + str(discriminator_loss[-1][1])\n",
    "              + ' | Fake: ' + str(discriminator_loss[-1][3])\n",
    "              + ' | Penalty: ' + str(discriminator_loss[-1][5])\n",
    "              + ' | Categorical: '+str(discriminator_loss[-1][2]))\n",
    "        np.savetxt(os.path.join(result_path, 'gan_d_loss_history.csv'), np.asarray(discriminator_loss)[:,0])\n",
    "        \n",
    "        print('G. Loss | Total: ' + str(generator_loss[-1][0])\n",
    "             + ' | Real/Fake: ' +  str(generator_loss[-1][1])\n",
    "             + ' | Categorical: ' + str(generator_loss[-1][2]))\n",
    "        np.savetxt(os.path.join(result_path, 'gan_g_loss_history.csv'), np.asarray(generator_loss)[:,0])\n",
    "        \n",
    "    if batch_num % sample_period == 0 and batch_num > 1:\n",
    "        generate_images(generator, result_path, epoch)\n",
    "        if discriminator_loss[-1][0] < 2.0 and generator_loss[-1][0] < 3.5:\n",
    "            try:\n",
    "                generator_model.save_weights(os.path.join(result_path, 'gan_g_weights{}.h5'.format(epoch)))\n",
    "                discriminator_model.save_weights(os.path.join(result_path, 'gan_d_weights{}.h5'.format(epoch)))\n",
    "            except:\n",
    "                print('Weights could not be saved')\n",
    "        else:\n",
    "            print('Loss was too high - weights not saved')\n",
    "            generator_model.optimizer.lr *= 0.3\n",
    "            discriminator_model.optimizer.lr *= 0.3\n",
    "            \n",
    "        epoch += 1\n",
    "        print('\\nEpoch ' + str(epoch))\n",
    "    \n",
    "    if len(imgs) == batch_size:\n",
    "        # smooth positive labels only\n",
    "        positive_y = np.random.normal(1, 0.2, size=(batch_size, 1)) + 1\n",
    "        negative_y = np.zeros((batch_size, 1))\n",
    "    \n",
    "        # train discriminator with real, generated, and perturbed images\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, z_length))\n",
    "        gen_types = np.eye(num_types)[np.random.choice(num_types, batch_size)]\n",
    "        discriminator_loss.append(\n",
    "            discriminator_model.train_on_batch([imgs, noise, gen_types],\n",
    "                                               [positive_y, types, negative_y, gen_types, dummy_y, gen_types]))\n",
    "\n",
    "        # train generator to maximize discriminator loss\n",
    "        noise2 = np.random.normal(0, 1, size=(batch_size, z_length))\n",
    "        gen_types2 = np.eye(num_types)[np.random.choice(num_types, batch_size)]\n",
    "        generator_loss.append(\n",
    "            generator_model.train_on_batch([noise2, gen_types2], [positive_y, gen_types2]))\n",
    "        \n",
    "    batch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Download Ken Sugimori art from [bulbapedia](https://archives.bulbagarden.net/wiki/Category:Ken_Sugimori_Pok%C3%A9mon_artwork), augment\n",
    "2. Use [DRAGAN](https://github.com/kodalinaveen3/DRAGAN) to generate new pokemon art\n",
    "3. Use PIL to combine random char-rnn generated card properties and DRAGAN generated art, using [templates](https://pokemoncardresources.deviantart.com/gallery/51274687/Resources-Classic)\n",
    "\n",
    "## Download from Bulbapedia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cat_name = 'Ken_Sugimori_Pokémon_artwork'\n",
    "data_dir = '/home/ubuntu/fastai-data/pokemon_img'\n",
    "png_path = os.path.join(data_dir, 'pngs')\n",
    "img_width = 256"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import mwclient, requests, shutil\n",
    "\n",
    "site = mwclient.Site('archives.bulbagarden.net')\n",
    "category = site.Categories[cat_name]\n",
    "filenames = (x.page_title for x in category.members(namespace=6))\n",
    "for file in filenames:\n",
    "    file_url = 'http://archives.bulbagarden.net/w/index.php?title=Special:FilePath&file={}&width={}'.format(\n",
    "            file, img_width)\n",
    "    r = requests.get(file_url, stream=True)\n",
    "    if not r.status_code == 200:\n",
    "        print('Requested width is bigger than source - downloading full size')\n",
    "        file_url = 'http://archives.bulbagarden.net/w/index.php?title=Special:FilePath&file={}'.format(file)\n",
    "        r = requests.get(file_url, stream=True)\n",
    "    print('Thumbnail found')\n",
    "    if r.status_code == 200:\n",
    "        print('Saving file '+file)\n",
    "        output_filepath = os.path.join(png_path, file.replace(' ','_'))\n",
    "        with open(output_filepath, 'wb+') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with hypergan card images because keras is soooo slow\n",
    "\n",
    "- Seriously, raw tensorflow is like at least 10 times faster than keras for this. Also keras keeps making me tweak the hyperparameters every few epochs to avoid nans. I really should learn pytorch soon. Maybe my generator and discriminator architectures are overkill, but the double evaluation for each model and the gradient penalty are very inefficient in keras.\n",
    "\n",
    "- Looks like hypergan isn't very effective for this task either - discriminator or generator loss goes to 0 after just a few epochs, no matter what hyperparameters I try. May need more data augmentation, and it's easier to do that in keras."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mkdir hypergan_pokemon\n",
    "cd hypergan_pokemon\n",
    "# Train a 128x128 gan with batch size 128 on a folder of pngs\n",
    "hypergan new hypergan_pokemon\n",
    "# edit to increase number of parameters in model\n",
    "hypergan train ~/fastai-data/pokemon_img/train -s 128x128x3 --classloss -b 128 --resize --sampler static_batch --sample_every 10 --save_samples --save_every 100 --noviewer --device '/gpu:0' -c hypergan_pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basically copied [makegirlsmoe](https://makegirlsmoe.github.io/assets/pdf/technical_report.pdf)'s architecture and [pytorch DRAGAN](https://github.com/jfsantos/dragan-pytorch/blob/master/dragan.py) / [keras WGAN-GP](https://github.com/farizrahman4u/keras-contrib/blob/master/examples/improved_wgan.py)'s code, because I don't know what I'm doing that well.\n",
    "- GANs (Generative Adverserial Networks) use a competition between a generator and discriminator neural network to gradually make the generator output match existing samples\n",
    "    - GANs are inefficient in pure keras - probably good to learn pytorch sometime\n",
    "    - also don't combine learning a new generator architecture (resnet), a new concept (GANs), and an experimental GAN architecture (DRAGAN) at the same time next time. Especially do not do this all while using the entire data set for training rather than samples.\n",
    "    - Every batch, first train the discriminator network to correctly classify real and generated images\n",
    "        - discriminator is a convolutional image classifier network that classifies images as real or fake (binary).\n",
    "        - discriminator network is actually stacked generator and discriminator, but only discriminator weights are being trained. Input a batch of real images and a batch of noise, feed the noise through the generator and then through the discriminator (real images go straight to discriminator), and optimize discriminator to classify real images as real and generated images as fake.\n",
    "    - then, train the generator network to fool the discriminator network\n",
    "        - generator takes noise vector and generates image. In this case we reshape a noise vector into an image, use a dense layer to increase a low resolution image's depth (# of channels), convolutional layers to create features, and pixel shuffles to increase resolution by decreasing depth, until we end up with an appropriate image.\n",
    "        - generator network is also stacked generator and discriminator, but only the generator weights are trained now. Input a batch of noise, feed it through the generator and discriminator, and optimize the generator so the discriminator classifies the generated images as real (by feeding it the opposite labels).\n",
    "    - Overall discriminator loss should be lower than generator loss (so generator always has a somewhat accurate goal);  about 0.1-1 discriminator loss and 2-7 generator loss may be good. Loss should not converge but reach equilibrium, as increasing discriminator accuracy leads to better generator which lowers discriminator accuracy and vice versa.\n",
    "    - DRAGAN adds a gradient penalty to the discriminator to make it easier to train. Creates perturbed images (adding noise vectors to real images) and adds a penalty to the loss function proportional to the gradient of the discriminator at those perturbed images (should make discriminator more linear?)\n",
    "    - GAN tricks is very helpful, and follow pytorch DRAGAN implementation as closely as possible\n",
    "        - Remember example implementations and overview papers often don't show things like dropout\n",
    "- Resnets use skip connections to make networks more linear and help with deep networks\n",
    "    - resblocks have skip connections, where output from earlier layer is elementwise summed with output from later layer. See resnet paper. This helps make deep networks more linear and generalizable\n",
    "    - pixel shuffle is used to increase resolution, by making an image tensor wider (higher res) but shallower (fewer channels). See [subpixel](https://github.com/tetrachrome/subpixel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
