{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Intro\n",
    "\n",
    "- Finish 60 minute blitz from pytorch tutorials\n",
    "- Implement character embedding rnn in pytorch, based on theano section of fastai lesson 6\n",
    "\n",
    "### Tensor operations\n",
    "- Basically all the numpy ndarray functions are supported\n",
    "- Can be converted from and to numpy - np and torch tensors and then linked and modifications to one affect the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6646  0.0988  0.7094\n",
      " 0.9374  0.2605  0.5926\n",
      " 0.8577  0.9507  0.8256\n",
      " 0.5294  0.1175  0.5782\n",
      " 0.1380  0.2708  0.8770\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66459256  0.09881883  0.70937097]\n",
      " [ 0.93740928  0.26048753  0.59258026]\n",
      " [ 0.85772985  0.95066363  0.82564968]\n",
      " [ 0.52941418  0.11749824  0.57819259]\n",
      " [ 0.13795471  0.27078828  0.87696105]]\n"
     ]
    }
   ],
   "source": [
    "b = x.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6646  0.0988  0.7094\n",
      " 0.9374  0.2605  0.5926\n",
      " 0.8577  0.9507  0.8256\n",
      " 0.5294  0.1175  0.5782\n",
      " 0.1380  0.2708  0.8770\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd - Variables and Functions\n",
    "- Variable wraps a tensor, has `.data` for raw Tensor data, `.grad` for gradient (computed after calling `backward()` on an output Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddConstantBackward object at 0x7f71327f17c8>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MulConstantBackward object at 0x7f71327f18b8>\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute gradients\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "- a neural network is a class (subclassing `nn.Module`) that defines a set of e.g. `nn.Linear` layers and a method `forward(input)` that takes a Variable input and outputs another Variable (`backward()` created by autograd)\n",
    "- `net.parameters` are learnable weights of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input channel, 6 output channel, 5x5 convolution\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120) # 400 inputs, 120 outputs\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # apply conv1, relu, and 2x2 max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x)) # flatten (x.view reshapes tensor)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except batch size\n",
    "        flattened = 1\n",
    "        for i in size:\n",
    "            flattened *= i\n",
    "        return flattened\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 37.3953\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "target = Variable(torch.arange(1, 11))\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "out = net(input)\n",
    "loss = nn.MSELoss()(out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 36.6001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    optimizer.zero_grad() # zero gradients so they can be recalculated\n",
    "    out = net(input) # run forward pass\n",
    "    loss = nn.MSELoss()(out, target) # calculate loss\n",
    "    loss.backward() # get gradients with respect to loss\n",
    "    optimizer.step() # step weights\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = '/home/ubuntu/fastai-data/rvb/scripts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1902635\n",
      "char list: \n",
      " !\"#$%&'()*+,-./0123456789:;<>?[]abcdefghijklmnopqrstuvwxyz ¡¿àáäèéêíñóöú\n",
      "total chars: 75\n"
     ]
    }
   ],
   "source": [
    "# load text and get character set\n",
    "text = open(path).read().lower()[:]\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('char list: '+''.join(chars))\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', vocab_size)\n",
    "\n",
    "# create character to index lookup table\n",
    "char_indices = dict((character, index) for index, character in enumerate(chars))\n",
    "# turn text into list of character indices\n",
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 64 # predict next character from preceding n characters\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(len(idx) - maxlen + 1):\n",
    "    sequences.append(idx[i:i+maxlen]) # get every sequence of length maxlen in text\n",
    "    next_chars.append(idx[i+1:i+maxlen+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(sequence):\n",
    "    categorical = np.zeros((len(sequence), vocab_size))\n",
    "    for li in range(len(sequence)):\n",
    "        letter_id = sequence[li]\n",
    "        categorical[li,letter_id] = 1\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to 2d numpy arrays - each row is a sequence, column values are characters in that sequence\n",
    "sequences = np.concatenate([one_hot(seq) for seq in sequences[:-2]])\n",
    "next_chars = np.concatenate([one_hot(seq) for seq in next_chars[:-2]])\n",
    "\n",
    "sequences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN (\n",
      "  (i2h): Linear (75 -> 256)\n",
      "  (h2h): Linear (256 -> 256)\n",
      "  (h2o): Linear (256 -> 75)\n",
      "  (dropout): Dropout (p = 0.05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=vocab_size, hidden_size=256):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(vocab_size, hidden_size) # input to hidden layer\n",
    "        self.h2h = nn.Linear(256, 256) # hidden to hidden layer\n",
    "        self.h2o = nn.Linear(256, vocab_size) # hidden to output layer\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = F.relu(self.i2h(x))\n",
    "        hidden = x + self.dropout(F.relu(self.h2h(hidden)))\n",
    "        output = F.softmax(self.h2o(hidden))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "rnn = CharRNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling function for model\n",
    "def sample(rnn, start_text='\\n'):\n",
    "    idx = char_indices[start_char[-1]]\n",
    "    prev_char = Variable(one_hot(idx))\n",
    "    output_text = start_text\n",
    "    \n",
    "    hidden = rnn.init_hidden()\n",
    "    for i in range(length):\n",
    "        output_char, hidden = rnn(prev_char, hidden)\n",
    "        prev_char = output_char\n",
    "        start_text += char_indice\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training function for one sequence\n",
    "def train(rnn, input_sequence, target_sequence, loss_func=nn.NLLLoss(), lr=0.01):\n",
    "    hidden = rnn.init_hidden() # initialize hidden state to 0s\n",
    "    rnn.zero_grad() # reset gradients so they can be recalculated\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(input_sequence)):\n",
    "        output_char, hidden = rnn(input[i], hidden) # rnn predicts next character\n",
    "        loss += loss_func(output_char, target[j]) # add loss for that character to total loss\n",
    "            \n",
    "    loss.backward() # recalculate gradients\n",
    "        \n",
    "    for p in rnn.parameters(): # update weights\n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "        \n",
    "    print('loss: '+str(loss.data[0] /input_sequence.size()[0])) # print average per-character loss\n",
    "    print('sample: '+sample(net)) # print sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "input = Variable(torch.from_numpy(sequences))\n",
    "target = Variable(torch.from_numpy(next_chars))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
